{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Test/Generate\" data-toc-modified-id=\"Test/Generate-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Test/Generate</a></span></li><li><span><a href=\"#[TOFIX]-Training---Estimator\" data-toc-modified-id=\"[TOFIX]-Training---Estimator-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>[TOFIX] Training - Estimator</a></span></li><li><span><a href=\"#[TOFIX]-Contrib-GAN\" data-toc-modified-id=\"[TOFIX]-Contrib-GAN-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>[TOFIX] Contrib GAN</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#tf.enable_eager_execution()\n",
    "import functools\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dcgan\n",
    "import gan_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path.home() / \"Documents/models/tf_playground/dcgan\"\n",
    "#model_dir = Path(\"/notebooks/models/dcgan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist_config.yaml', 'r') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conf = config['data']\n",
    "data_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = data_conf['input_shape']\n",
    "Z_SHAPE = (data_conf['z_size'], )\n",
    "BATCH_SIZE = config['training']['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "    images = images.reshape(images.shape[0], *INPUT_SHAPE).astype('float32')\n",
    "    images = (images - 127.5) / 127.5 # Normalize the images to [-1, 1]\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(data_conf['buffer_size'])\\\n",
    "                                                                .batch(BATCH_SIZE).take(512)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).take(128).batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_input = tf.placeholder(tf.float32, name='real_input')\n",
    "input_noise = tf.placeholder(tf.float32, shape=(None, data_conf['z_size']), name='input_noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = dcgan.get_generator(Z_SHAPE, **config['model']['generator'])\n",
    "discriminator = dcgan.get_discriminator(INPUT_SHAPE, **config['model']['discriminator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO validate generator output shape equal to discriminator input shape\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_real = discriminator(real_input)\n",
    "G_z = generator(input_noise)\n",
    "\n",
    "D_fake = discriminator(G_z)\n",
    "\n",
    "# Generator and Discrimnator Losses\n",
    "G_loss = dcgan.generator_loss(D_fake)\n",
    "D_loss = dcgan.discriminator_loss(D_real, D_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator_optimizer = tf.train.AdamOptimizer(config['training']['generator']['learning_rate'])\n",
    "#discriminator_optimizer = tf.train.AdamOptimizer(config['training']['discriminator']['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_discriminator = dcgan.get_discriminator_train(discriminator, D_loss,\n",
    "                                                       config['training']['discriminator'])\n",
    "train_generator = dcgan.get_generator_train(generator, G_loss,\n",
    "                                                       config['training']['generator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "CHECKPOINT_STEPS = config['training']['checkpoint_steps']\n",
    "PLOT_SAMPLE_SIZE = config['training']['plot_sample_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_WEIGHTS = False\n",
    "\n",
    "iterator = train_dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # tensorboard\n",
    "    summ_writer = tf.summary.FileWriter(str(model_dir), sess.graph)\n",
    "    gen_loss_summary = tf.summary.scalar('gen_loss', G_loss)\n",
    "    disc_loss_summary = tf.summary.scalar('disc_loss', D_loss)\n",
    "    \n",
    "    test_gen_images = tf.placeholder(tf.float32, name='gen_images', shape=(1, 288, 288, 4))\n",
    "    summary_imgs = tf.summary.image(\"plot\", test_gen_images)\n",
    "    \n",
    "    z_fixed = np.random.normal(size=(PLOT_SAMPLE_SIZE, data_conf['z_size']))\n",
    "    in_noise = tf.random_normal([BATCH_SIZE, data_conf['z_size']])\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if LOAD_WEIGHTS:\n",
    "        generator.load_weights(str(model_dir / \"generator\"))\n",
    "        discriminator.load_weights(str(model_dir / \"discriminator\"))\n",
    "        \n",
    "    for epoch in tqdm(range(EPOCHS)):\n",
    "        sess.run(iterator.initializer)\n",
    "        \n",
    "        # train across entire dataset\n",
    "        batch_num = 0\n",
    "        while True:\n",
    "            try:\n",
    "                # TODO cleaner way to connect input_image and noise to dataset and function\n",
    "                input_image = sess.run(next_element)\n",
    "                noise = sess.run(in_noise)\n",
    "                g_loss_summ, d_loss_summ, _, _ = sess.run([gen_loss_summary, disc_loss_summary, \n",
    "                                                           train_discriminator, train_generator], \n",
    "                        feed_dict={real_input: input_image, input_noise: noise})\n",
    "                batch_num += 1\n",
    "                if batch_num >= 1:\n",
    "                     break\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        \n",
    "        summ_writer.add_summary(g_loss_summ, epoch)\n",
    "        summ_writer.add_summary(d_loss_summ, epoch)\n",
    "        \n",
    "        predictions = sess.run(generator(test_noise), {test_noise: z_fixed})\n",
    "        plot_buf = gan_utils.display_prediction(predictions, epoch)\n",
    "        gen_images = tf.image.decode_png(plot_buf.getvalue(), channels=4)\n",
    "        gen_images = tf.expand_dims(gen_images, 0)\n",
    "        \n",
    "        summary_imgs_val = sess.run(summary_imgs, {test_gen_images:gen_images.eval()})\n",
    "        summ_writer.add_summary(summary_imgs_val, epoch)\n",
    "        \n",
    "        # saving checkpoint\n",
    "        if (epoch + 1) % CHECKPOINT_STEPS == 0:\n",
    "            # TODO rely on TF instead\n",
    "            # TODO Validate if training params are also loaded\n",
    "            # TODO export by epoch\n",
    "            generator.save_weights(str(model_dir / \"generator\"))\n",
    "            discriminator.save_weights(str(model_dir / \"discriminator\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(str(model_dir / \"generator\"))\n",
    "test_noise = np.random.rand(PLOT_SAMPLE_SIZE, data_conf['z_size'])\n",
    "predictions = generator.predict(test_noise)\n",
    "gan_utils.display_prediction(predictions, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data-science]",
   "language": "python",
   "name": "conda-env-data-science-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "30",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "592px",
    "left": "0px",
    "right": "1056.75px",
    "top": "87px",
    "width": "143.25px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
